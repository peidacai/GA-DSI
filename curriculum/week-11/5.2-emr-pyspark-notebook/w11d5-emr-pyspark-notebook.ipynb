{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Jupyter + PySpark on EMR\n",
    "\n",
    "Week 11 | Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Launch a Jupyter notebook on EMR\n",
    "- Utilize PySpark in a Jupyter notebook on EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min  | [Opening](#opening)  | Spark + EMR Review  |\n",
    "| 15 min  | [Cluster Launch Review](#cluster)   | Launching a Cluster Review  |\n",
    "| 15 min  | [Installing Conda](#conda)   | Installing Anaconda on AWS |\n",
    "| 15 min | [Setting up a Jupyter Profile](#nb) | Setting a Jupyter Profile |\n",
    "| 15 min | [Getting a Spark Context](#sc) | Getting a Spark Context |\n",
    "| 15 min | [PySpark Operations in Jupyter](#pyspark) | PySpark Operations in Jupyter |\n",
    "| 5 min | [Conclusion](#conclusion) | Conclusion |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Opening (5 mins)\n",
    "\n",
    "We've seen previously:\n",
    "- How to launch an EMR cluster on AWS\n",
    "- How to work with Jupyter notebooks on your local machine\n",
    "- How to work with PySpark on a VM\n",
    "\n",
    "In this lesson, we'll learn how to tie it all together and launch an EMR cluster with the familiar Jupyter notebook interface and the power of PySpark at your disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Launching a Cluster on AWS Review\n",
    "\n",
    "\n",
    "Last week, we discussed how to launch an EMR cluster with a Hadoop environment installed. Today, we going to launch an EMR cluster with a Spark environment.\n",
    "\n",
    "Step 1: Log into the AWS dashboard and select EMR\n",
    "\n",
    "<img src=\"http://i.imgur.com/eSjYvn6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 2: Select Spark environment\n",
    "\n",
    "<img src=\"http://i.imgur.com/vt4DQDc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make sure to select the following options:\n",
    "<img src=\"http://i.imgur.com/vk9p5Td.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://i.imgur.com/ddQz3py.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make sure you have a working keypair; if not make sure to set one up. (They are region-specific.)\n",
    "\n",
    "Click to create the cluster. It will take several minutes to boot up.\n",
    "\n",
    "You will need to make sure you have port 22, 443, and 8888 open so you can ssh into the box. This is done through the EC2 tab -> Security Groups -> Inbound\n",
    "\n",
    "<img src=\"http://i.imgur.com/fa3dz5R.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once those are set and the cluster is ready, select the cluster from the cluster list by clicking on its name.\n",
    "\n",
    "Next, click 'Enable Web Connection'\n",
    "\n",
    "<img src=\"http://i.imgur.com/IFTc35a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Follow the directions to get a web connection enabled. You will need to open an ssh tunnel and get foxy proxy set up and working.\n",
    "\n",
    "<img src=\"http://i.imgur.com/f5q0ZE8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When that is all working you should see the AWS dashboard 'Connections' be clickable.\n",
    "\n",
    "<img src=\"http://i.imgur.com/8pDJQpI.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installing Anaconda\n",
    "\n",
    "Now open a new terminal tab and ssh in to your EMR instance. You should see the EMR ascii art.\n",
    "Once on the machine run the following commands to download and install Anaconda on the machine.\n",
    "\n",
    "```bash\n",
    "wget http://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh\n",
    "bash Anaconda2-4.1.1-Linux-x86_64.sh\n",
    "```\n",
    "Follow the prompts to accept the user agreement and Anaconda will be installed. Accept all the defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once that is complete, set the PATH so that Anaconda is the default Python:\n",
    "\n",
    "```bash\n",
    "export PATH=\"/home/hadoop/anaconda2/bin\":$PATH\n",
    "```\n",
    "\n",
    "When you type \"conda\" on the command line you should now get the conda help documentation.\n",
    "\n",
    "We'll now move on to installing the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installing Jupyter Notebook \n",
    "\n",
    "In the EMR instance's terminal type the following:\n",
    "\n",
    "```bash\n",
    "ipython\n",
    "```\n",
    "This will launch an iPython instance. In that iPython instance, type the following:\n",
    "\n",
    "```python\n",
    "from IPython.lib import passwd\n",
    "passwd()\n",
    "```\n",
    "You will be prompted to enter a password and to confirm it. Do that. Following this an sha key will be displayed. Copy the entire string.\n",
    "\n",
    "Type exit to leave iPython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now set up an IPython profile. Type the following to generate this.\n",
    "\n",
    "```bash\n",
    "jupyter notebook --generate-config\n",
    "```\n",
    "\n",
    "Leave that for now as we need to create a self-signed cert. From your home directory:\n",
    "\n",
    "```bash\n",
    "mkdir certs\n",
    "cd certs\n",
    "sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's go back and configure our Jupyter profile configuration.\n",
    "\n",
    "```bash\n",
    "cd ~/.jupyter\n",
    "vim jupyter_notebook_config.py\n",
    "```\n",
    "\n",
    "This will open the config file. Next we'll make a few changes. Near the top of the file paste in the following. Remember when using vim, you should hit \"i\" to insert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then paste the following:\n",
    "\n",
    "```bash\n",
    "c = get_config()\n",
    "# Kernel config\n",
    "c.IPKernelApp.pylab = 'inline'  # if you want plotting support always in your notebook\n",
    "# Notebook config\n",
    "c.NotebookApp.certfile = u'/home/hadoop/certs/mycert.pem' #location of your certificate file\n",
    "c.NotebookApp.ip = '*'\n",
    "c.NotebookApp.open_browser = False\n",
    "c.NotebookApp.password = u'sha1:12121...'  #the encrypted password we generated earlier\n",
    "c.NotebookApp.port = 8888\n",
    "```\n",
    "Then save the file and exit. (ESC -> ':wq' -> Enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Now we can almost launch our notebook, but first do the following:\n",
    "\n",
    "```bash\n",
    "cd ~/\n",
    "export SPARK_HOME=\"/usr/lib/spark\"\n",
    "mkdir notebooks\n",
    "cd notebooks\n",
    "jupyter notebook\n",
    "```\n",
    "This will launch your notebook, but you won't see anything (which is good). To see our notebook, we need to go to the url listed on the dashboard near \"Master public DNS\". Copy this and paste it into your browser as follows where <aws_dns> is what you copied:\n",
    "\n",
    "```bash\n",
    "https://<aws_dns>:8888\n",
    "```\n",
    "\n",
    "You will get a horrifying warning that you can ignore. Click on 'Advanced' and then 'Proceed to...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You will see a login screen like the following:\n",
    "\n",
    "<img src=\"http://i.imgur.com/mLP3nHB.png\">\n",
    "\n",
    "Use the password you used to generate your sha key earlier. If you did everything ok, you will be confronted my the joyous sight of a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If all went as planned, you have a new Jupyter instance running. Create a new notebook and open that. Run the following commands in the first cell to get a spark context running:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# get spark context\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.1-src.zip'))\n",
    "filename = os.path.join(spark_home, 'python/pyspark/shell.py')\n",
    "exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\n",
    "```\n",
    "\n",
    "Run the cell and you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://i.imgur.com/drWbsud.png\">\n",
    "\n",
    "You should also be able to type in 'sc' hit return and see that you have a spark context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PySpark Operations in Jupyter\n",
    "\n",
    "\n",
    "Let's try some basic operations:\n",
    "```python\n",
    "# distribute our data\n",
    "txt = sc.parallelize([('a', 1), ('b', 1), ('a', 1), ('a', 1), ('c', 2), ('d', 10)])\n",
    "\n",
    "# reduce the data by key\n",
    "red = txt.reduceByKey(lambda a, b: a+b).cache()\n",
    "\n",
    "# view the results\n",
    "print red.take(4)\n",
    "\n",
    "# setting results in a DataFram\n",
    "df = pd.DataFrame(red.take(4))\n",
    "df.set_index(0, inplace=1)\n",
    "df.index.name = None\n",
    "df.columns = ['count']\n",
    "df.sort_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Now with an external text file:\n",
    "\n",
    "```python\n",
    "# load the file\n",
    "text_file = sc.textFile('s3://elasticmapreduce/samples/wordcount/input/0001')\n",
    "\n",
    "# view the head\n",
    "text_file.take(5)\n",
    "\n",
    "# calculate the word counts\n",
    "word_counts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "# run the operation\n",
    "word_counts.collect()\n",
    "\n",
    "# now try putting the results in a DataFrame\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lesson we've seen how to how to launch an EMR cluster that leverages the power of PySpark in a Jupyter notebook.\n",
    "\n",
    "Additional PySpark Resources:\n",
    "- [PySpark documentation](http://spark.apache.org/docs/2.0.0/api/python/index.html)\n",
    "- [From Pandas DataFrame to Spark DataFrames](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)\n",
    "- [PySpark DataFrame](http://blog.brakmic.com/data-science-for-losers-part-5-spark-dataframes/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {
    "height": "203px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
