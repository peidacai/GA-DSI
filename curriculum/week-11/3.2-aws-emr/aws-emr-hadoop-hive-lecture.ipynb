{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) AWS Elastic Map Reduce\n",
    "Week 11 | Lesson 3.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- spin up a cluster on AWS\n",
    "- browse HDFS using Hadoop User Environment\n",
    "- launch a HIVE Shell and execute HIVE queries on an EMR cluster\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- sign in to your AWS console\n",
    "- run HIVE queries on the VM\n",
    "- explain what Hadoop is and how it works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min | [Opening](#opening) | Opening |\n",
    "| 5 min | [Introduction](#introduction) | Intro to EMR |\n",
    "| 15 min | [Guided-practice](#guided-practice) | EMR cluster |\n",
    "| 5 min | [Ind-practice](#ind-practice) | EMR cluster |\n",
    "| 20 min | [Guided-practice](#guided-practice) | Configure Web Connection |\n",
    "| 20 min | [Guided-practice](#guided-practice) | Hadoop User Environment [HUE] |\n",
    "| 20 min | [Ind-practice](#ind-practice) | Independent practice |\n",
    "| 5 min | [Conclusion](#conclusion) | Conclusion |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 min)\n",
    "\n",
    "So far we've used two important AWS services: EC2 and S3.\n",
    "\n",
    "Today we'll learn how to spin up a *cluster* on Amazon. \n",
    "\n",
    "**Check:** What is a cluster?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: a cluster is a set of several computers communicating with one another and working together to solve a problem.\n",
    "\n",
    "**Check:** What is a typical topology for a Big Data computing cluster?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: a computing cluster generally has a star topology, with one master node and several job and task nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to EMR (5 min)\n",
    "\n",
    "Amazon Elastic MapReduce was introduce in April 2009 to automate _provisioning_ of the Hadoop cluster, running and terminating jobs, and handling data transfer between EC2(VM) and S3(Object Storage). It simplifies the management of a Hadoop cluster, making it available to anyone at the click of a button.\n",
    "\n",
    "EMR also supports spot instances, which are a good way to lower costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EMR offers several pre-installed software packages including:\n",
    "\n",
    "- Hadoop\n",
    "- HBase\n",
    "- Pig\n",
    "- Hive\n",
    "- Hue\n",
    "- Spark\n",
    "and many others.\n",
    "\n",
    "**Check:** Which of these have you already encountered on your local VM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Hadoop, Hive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EMR Pricing\n",
    "\n",
    "EMR pricing is based on the type of instances forming the cluster and its divided in tiers.\n",
    "\n",
    "Costs are calculated in hourly increments, so if we plan to use the cluster for 2 sessions of half an hour, we should have it up for 1 hour consecutively instead of spinning it up and down twice.\n",
    "\n",
    "EMR is not included in the AWS free tier that you've used in the previous class, so it's always a good practice to do some price checking before you set up a cluster.\n",
    "\n",
    "We can use the [AWS cost calculator](https://calculator.s3.amazonaws.com/index.html) to estimate the cost of a 3 node cluster with medium size instances `(m3.xlarge)`. The image below shows the cost for 1 hour: it's slightly more than a dollar.\n",
    "\n",
    "![](./assets/images/emrcost.png)\n",
    "\n",
    "If we were to keep the cluster alive for a month, that starts to add up -- so spin down this cluster when you're done today!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## EMR cluster (15 min)\n",
    "\n",
    "Let's spin up an EMR cluster with Hive and use it to perform a simple word count using Hive. We will be following the [example provided by Amazon here](http://docs.aws.amazon.com//ElasticMapReduce/latest/ManagementGuide/emr-gs.html).\n",
    "\n",
    "Log into AWS and first go to the S3 interface. Name a bucket, then click into it.\n",
    "\n",
    "#### 1. Prerequisites\n",
    "As a first step we will create 2 folders in an s3 bucket of ours and call them:\n",
    "- input\n",
    "- output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can do this manually:\n",
    "\n",
    "![](./assets/images/bucket.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or via the command line:\n",
    "\n",
    "```bash\n",
    "aws s3 ls\n",
    "\n",
    "aws s3 mb s3://bucket-name\n",
    "# you can remove it using aws s3 rb s3://bucket-name\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then go to the EMR service page, under the Analytics category:\n",
    "\n",
    "![](./assets/images/emr.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.  Launch Cluster\n",
    "\n",
    "![](./assets/images/clusterstart.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A couple important notes:\n",
    "\n",
    "- Choose the key pair you have already stored on your computer.\n",
    "- Use release 4.6.0, not the default. (This is so our example script will work; in general you can use the latest release.)\n",
    "\n",
    "![](./assets/images/clusterstarting.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "AWS will also export the corresponding AWSCLI command for you, which in this case is something like:\n",
    "\n",
    "```bash\n",
    "aws emr create-cluster \\\n",
    "    --applications Name=Ganglia Name=Hadoop Name=Hive Name=Hue Name=Mahout Name=Pig \\\n",
    "    --ec2-attributes '{\"KeyName\":\"MyFirstKey\",\n",
    "                       \"InstanceProfile\":\"EMR_EC2_DefaultRole\",\n",
    "                       \"SubnetId\":\"subnet-9408e3cc\",\n",
    "                       \"EmrManagedSlaveSecurityGroup\":\"8c01ffea\",\n",
    "                       \"EmrManagedMasterSecurityGroup\":\"sg-8f01ffe9\"}'\n",
    "    --service-role EMR_DefaultRole --enable-debugging --release-label emr-4.6.0 \\\n",
    "    --log-uri 's3n://aws-logs-408624971132-us-west-2/elasticmapreduce/' \\\n",
    "    --name 'My GA test cluster' \\\n",
    "    --instance-groups '[{\"InstanceCount\":1,\n",
    "                         \"InstanceGroupType\":\"MASTER\",\n",
    "                         \"InstanceType\":\"m3.xlarge\",\n",
    "                         \"Name\":\"Master Instance Group\"},\n",
    "                        {\"InstanceCount\":2,\n",
    "                         \"InstanceGroupType\":\"CORE\",\n",
    "                         \"InstanceType\":\"m3.xlarge\",\n",
    "                         \"Name\":\"Core Instance Group\"}]' \\\n",
    "    --region us-west-2\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As with EC2, we can list the clusters in the Cluster List pane:\n",
    "\n",
    "![](./assets/images/clusterlist.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The cluster can take 5-10 minutes to boot completely. In the meantime, let's do a few review checks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What exercise did we do this morning with Hive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> word count in books, log parsing using SERDE command.\n",
    "\n",
    "**Check:** How do you connect to an instance on EC2?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ssh -i key ec2-user@instance_dns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once the cluster is ready we will see it in green:\n",
    "\n",
    "![](./assets/images/clusterready.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3. Prepare sample data and script\n",
    "\n",
    "We will analyze log data, using a dataset and computing power that is entirely in the cloud. Our local machine's only role is to set up the system.\n",
    "\n",
    "The sample data is a series of Amazon CloudFront web distribution log files. The data is stored in Amazon S3 at `s3://us-west-2.elasticmapreduce.samples` (make sure the region, e.g. `us-west-2`, is your region).\n",
    "\n",
    "Each entry in the CloudFront log files provides details about a single user request in the following format:\n",
    "\n",
    "    2014-07-05 20:00:00 LHR3 4260 10.0.0.15 GET eabcd12345678.cloudfront.net /test-image-1.jpeg 200 - Mozilla/5.0%20(MacOS;%20U;%20Windows%20NT%205.1;%20en-US;%20rv:1.9.0.9)%20Gecko/2009040821%20IE/3.0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A sample Hive script is provided here:\n",
    "\n",
    "    s3://us-west-2.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q\n",
    "\n",
    "This script:\n",
    "\n",
    "- Creates a Hive table named cloudfront_logs.\n",
    "- Reads the CloudFront log files from Amazon S3 using EMRFS and parses the CloudFront log files using the regular expression serializer/deserializer (RegEx SerDe).\n",
    "- Writes the parsed results to the Hive table cloudfront_logs.\n",
    "- Submits a HiveQL query against the data to retrieve the total requests per operating system for a given time frame.\n",
    "- Writes the query results to your Amazon S3 output bucket.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Hive code that creates the table looks like this:\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS cloudfront_logs ( \n",
    "\tDate Date, \n",
    "\tTime STRING, \n",
    "\tLocation STRING, \n",
    "\tBytes INT, \n",
    "\tRequestIP STRING, \n",
    "\tMethod STRING, \n",
    "\tHost STRING, \n",
    "\tUri STRING, \n",
    "\tStatus INT, \n",
    "\tReferrer STRING, \n",
    "\tOS String, \n",
    "\tBrowser String, \n",
    "\tBrowserVersion String \n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Here's the Hive code that parses the log files using the RegEx SerDe:\n",
    "\n",
    "```sql\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' \n",
    "WITH SERDEPROPERTIES ( \"input.regex\" = \"^(?!#)([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+[^\\(]+[\\(]([^\\;]+).*\\%20([^\\/]+)[\\/](.*)$\" ) LOCATION 's3://us-west-2.elasticmapreduce.samples/cloudfront/data/';\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The HiveQL query is:\n",
    "\n",
    "```sql\n",
    "SELECT os, COUNT(*) count FROM cloudfront_logs WHERE date BETWEEN '2014-07-05' AND '2014-08-05' GROUP BY os;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4. Process Sample Data\n",
    "\n",
    "Following the instructions [here](http://docs.aws.amazon.com//ElasticMapReduce/latest/ManagementGuide/emr-gs-process-sample-data.html) we can create a new step job based on the Hive script by adding a `step` and assigning input, output and script buckets.\n",
    "\n",
    "![](./assets/images/steppending.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/images/steprunning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5. Check results\n",
    "\n",
    "\n",
    "![](./assets/images/results.png)\n",
    "\n",
    "\n",
    "You can navigate your S3 bucket and check the results. There should be a new file, with the content:\n",
    "\n",
    "    Android    855\n",
    "    Linux      813\n",
    "    MacOS      852\n",
    "    OSX        799\n",
    "    Windows    883\n",
    "    iOS        794\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've just run a Hive script on EMR! Fun.\n",
    "\n",
    "**Check:** We have run a Hive script by defining a step in the EMR GUI. Do you think we could simply run Hive commands from the command line?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: yes, we could SSH into the master node and then launch Hive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## EMR cluster (5 min)\n",
    "Go ahead and SSH to your master node and launch Hive. They try to query the table you just created (`cloudfront_logs`).\n",
    "\n",
    "If you're having trouble connecting, you may need to add SSH access permissions to your security group:\n",
    "\n",
    "Click `Security groups for Master:`, click the `Inbound` tab, `Edit`, `Add Rule`, then select SSH for the `Type` and My IP for `Source`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Configure Web Connection (20 min)\n",
    "\n",
    "So far we have learned 2 ways of running Hive. Can you list them?\n",
    "> Answer:\n",
    ">\n",
    "- command line\n",
    "- script passed as step to EMR cluster\n",
    "\n",
    "We will now learn about HUE, or Hadoop User Interface, which is a great way to interact with a Hadoop cluster.\n",
    "\n",
    "Before we can do that, will have to go through one more step. The default security settings for EMR do not allow for external web connections to our cluster. In order to connect with a browser we will have to set up an _ssh tunnel_, i.e. have our browser communicate to the cluster via an encrypted channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luckily, Amazon provides us with simple instructions:\n",
    "\n",
    "![](./assets/images/webconnection.png)\n",
    "\n",
    "![](./assets/images/sshtunnel.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First we need to do a few things:\n",
    "\n",
    "1) If you haven't already, enable SSH access to our master node. This is done in the Security Groups pane of the EC2 services page. (See above.)\n",
    "\n",
    "![](./assets/images/securitygroups.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2) Add an inbound Custom TCP Rule, for ports 8888 and 50070, allowing access from your IP.\n",
    "\n",
    "> First try this WITHOUT step 3\n",
    "\n",
    "> 3) Install and configure Foxy-Proxy as explained [here](https://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-connect-master-node-proxy.html). (You want the \"Proxy\", \"Standard\" version.)\n",
    "\n",
    "Once we have enabled SSH access, we can go ahead and connect:\n",
    "\n",
    "```bash\n",
    "ssh -i ~/.ssh/MyFirstKey.pem -ND 8157 hadoop@<YOUR_MASTER_DNS>\n",
    "```\n",
    "\n",
    "Note that this command won't appear to do anything - it will quietly run and keep the tunnel alive.\n",
    "\n",
    "If the tunnel and Foxy-proxy are well configured, we should be able to connect to several web services. The one we are interested in is HUE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Hadoop User Environment [HUE] (20 min)\n",
    "\n",
    "[Hue](http://gethue.com/) aggregates the most common Apache Hadoop components into a single interface and targets the user experience. Its main goal is to have the users \"just use\" Hadoop without worrying about the underlying complexity or using a command line.\n",
    "\n",
    "It's accessible at the port 8888 of our master node, through an SSH tunnel. Since it's the first we use it, we'll have to set up a username and password. Choose whatever you'd like. (You'll probably be scrapping all of this at the end of the next lab.)\n",
    "\n",
    "    http://<YOUR_MASTER_DNS>.compute.amazonaws.com:8888/\n",
    "\n",
    "\n",
    "![](./assets/images/hueuser.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's also install all the examples:\n",
    "\n",
    "![](./assets/images/hueinstall.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we can finally open the Hue home page by clicking the home icon:\n",
    "\n",
    "![](./assets/images/huehome.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example 1: AWS Sample: CloudFront Logs\n",
    "\n",
    "Amongst the example there's one that looks familiar. It's the cloudfront sample logs script we've just executed in HIVE. Let's see what happens if we run it from HUE. Hit the EXECUTE button.\n",
    "\n",
    "We will see the log of the MR being executed:\n",
    "\n",
    "![](./assets/images/huecloudfront.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the results:\n",
    "\n",
    "![](./assets/images/huecfresults.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HUE also generates a nice chart for us:\n",
    "\n",
    "![](./assets/images/huechart.png)\n",
    "\n",
    "You can hit the `next` button to execute the next queries in the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, note that we can also explore the HDFS like we were doing on the local VM by pointing our browser to the 50070 port:\n",
    "\n",
    "    http://<YOUR_MASTER_DNS>:50070/dfshealth.html#tab-overview\n",
    "    \n",
    "![](./assets/images/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent practice (20 min)\n",
    "\n",
    "The HUE Home offers several other examples. In pairs choose an example and work through the code. Make sure you understand what it does when you execute it. Here are some questions to guide your discovery:\n",
    "\n",
    "- what tables are present?\n",
    "- how are they defined? what's the schema? how do you check it in HUE?\n",
    "- what does the query do?\n",
    "- how long does it take to execute?\n",
    "- how much data does it process?\n",
    "- what are the results?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 min)\n",
    "\n",
    "Today we have learned how to sping up a cluster on AWS and how to run HIVE queries on it using a script or using HUE.\n",
    "\n",
    "Make sure you terminate your cluster now:\n",
    "\n",
    "![](./assets/images/terminate.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also delete the buckets from S3, to avoid paying for storage space.\n",
    "\n",
    "![](./assets/images/deletebucket.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** Can you use this for any part of your capstone? What other use cases for EMR do you see in your future?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [AWS EMR tutorial](http://docs.aws.amazon.com//ElasticMapReduce/latest/ManagementGuide/emr-gs.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
